{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQN1WVfDc7Lq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "import pickle\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of random User-Agents\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\"\n",
        "]\n",
        "\n",
        "def get_random_user_agent():\n",
        "    return random.choice(USER_AGENTS)\n",
        "\n",
        "def random_sleep(min_seconds=2, max_seconds=5):\n",
        "    sleep_time = random.uniform(min_seconds, max_seconds)\n",
        "    print(f\"Waiting for {sleep_time:.2f} seconds...\")\n",
        "    time.sleep(sleep_time)\n",
        "\n",
        "def setup_driver(proxy=None, browser='chrome'):\n",
        "    if browser == 'chrome':\n",
        "        options = webdriver.ChromeOptions()\n",
        "        options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        options.add_argument(\"--incognito\")\n",
        "        options.add_argument(f\"user-agent={get_random_user_agent()}\")\n",
        "        if proxy:\n",
        "            options.add_argument(f'--proxy-server={proxy}')\n",
        "        driver = webdriver.Chrome(options=options)\n",
        "    elif browser == 'firefox':\n",
        "        from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
        "        options = FirefoxOptions()\n",
        "        options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--private\")\n",
        "        options.set_preference(\"general.useragent.override\", get_random_user_agent())\n",
        "        driver = webdriver.Firefox(options=options)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported browser!\")\n",
        "    return driver\n",
        "\n",
        "def get_page(driver, url, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            WebDriverWait(driver, 60).until(\n",
        "                EC.presence_of_element_located((By.CLASS_NAME, \"job_seen_beacon\"))\n",
        "            )\n",
        "            return True\n",
        "        except (TimeoutException, WebDriverException) as e:\n",
        "            print(f\"Error loading {url}: {e}. Attempt {attempt + 1} of {retries}.\")\n",
        "            if attempt < retries - 1:\n",
        "                random_sleep(5, 10)\n",
        "            else:\n",
        "                return False\n",
        "    return False\n",
        "\n",
        "def get_job_details(driver, job_url):\n",
        "    \"\"\"Extracts job details like location, salary, job type, and description.\"\"\"\n",
        "    try:\n",
        "        driver.execute_script(\"window.open('');\")\n",
        "        driver.switch_to.window(driver.window_handles[-1])\n",
        "        driver.get(job_url)\n",
        "        random_sleep(2, 4)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "        location_div = soup.find(\"div\", attrs={\"data-testid\": \"inlineHeader-companyLocation\"})\n",
        "        location = location_div.get_text(strip=True) if location_div else \"Unknown\"\n",
        "\n",
        "        job_info_container = soup.find(\"div\", id=\"salaryInfoAndJobType\")\n",
        "        if job_info_container:\n",
        "            span_list = job_info_container.find_all(\"span\", recursive=False)\n",
        "            salary = span_list[0].get_text(strip=True) if len(span_list) >= 1 else \"Not specified\"\n",
        "            job_type = span_list[1].get_text(strip=True) if len(span_list) >= 2 else \"Not specified\"\n",
        "        else:\n",
        "            salary = \"Not specified\"\n",
        "            job_type = \"Not specified\"\n",
        "\n",
        "        desc_div = soup.find(\"div\", id=\"jobDescriptionText\")\n",
        "        description = desc_div.get_text(separator=\"\\n\").strip() if desc_div else \"No full description\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_job_details: {e}\")\n",
        "        location, salary, description, job_type = \"Unknown\", \"Not specified\", \"No description\", \"Not specified\"\n",
        "    finally:\n",
        "        driver.close()\n",
        "        driver.switch_to.window(driver.window_handles[0])\n",
        "    return location, salary, description, job_type\n",
        "\n",
        "def save_cookies(driver, filename=\"cookies.pkl\"):\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(driver.get_cookies(), f)\n",
        "\n",
        "def load_cookies(driver, filename=\"cookies.pkl\"):\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, \"rb\") as f:\n",
        "            cookies = pickle.load(f)\n",
        "            for cookie in cookies:\n",
        "                try:\n",
        "                    driver.add_cookie(cookie)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error adding cookie: {e}\")\n",
        "\n",
        "def main():\n",
        "    job_titles = [\"data scientist\", \"machine learning engineer\", \"product manager\"]\n",
        "    locations = [\"New York\", \"San Francisco\", \"London\"]\n",
        "    filename = \"jobs_data.csv\"\n",
        "\n",
        "    driver = setup_driver(browser='chrome')\n",
        "    load_cookies(driver)\n",
        "\n",
        "    with open(filename, mode='a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=['Job Title', 'Location', 'Company', 'Job URL', 'Salary', 'Description', 'Job Type'])\n",
        "        writer.writeheader()\n",
        "\n",
        "        for job_title in job_titles:\n",
        "            for loc in locations:\n",
        "                for page in range(5):\n",
        "                    start = page * 10\n",
        "                    url = f\"https://www.indeed.com/jobs?q={'+'.join(job_title.split())}&l={'+'.join(loc.split())}&start={start}\"\n",
        "                    success = get_page(driver, url)\n",
        "                    if not success:\n",
        "                        break\n",
        "\n",
        "                    job_cards = driver.find_elements(By.CLASS_NAME, \"job_seen_beacon\")\n",
        "                    if not job_cards:\n",
        "                        break\n",
        "\n",
        "                    for card in job_cards:\n",
        "                        try:\n",
        "                            title = card.find_element(By.CSS_SELECTOR, \"h2.jobTitle span\").text.strip()\n",
        "                            company = card.find_element(By.CSS_SELECTOR, \"span[data-testid='company-name']\").text.strip()\n",
        "                            job_url = card.find_element(By.CSS_SELECTOR, \"h2.jobTitle a\").get_attribute(\"href\")\n",
        "                            location, salary, description, job_type = get_job_details(driver, job_url)\n",
        "\n",
        "                            writer.writerow({\"Job Title\": title, \"Location\": location, \"Company\": company, \"Job URL\": job_url, \"Salary\": salary, \"Description\": description, \"Job Type\": job_type})\n",
        "                            csvfile.flush()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting job data: {e}\")\n",
        "                    random_sleep(5, 10)\n",
        "\n",
        "    save_cookies(driver)\n",
        "    driver.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}